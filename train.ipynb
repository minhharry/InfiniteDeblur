{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms import v2\n",
    "import lpips\n",
    "\n",
    "from dataset import GoProDataset\n",
    "from model import BaseLineUnet\n",
    "from NAFnet_baseline.Baseline_arch import Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_of_files():\n",
    "    files = []\n",
    "    files.extend(glob.glob(\"**/*.py\", recursive=True))\n",
    "    files.extend(glob.glob(\"**/*.ipynb\", recursive=True))\n",
    "    data = {}\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text_content = f.read()\n",
    "        data[file] = text_content\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPerceptualLoss(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.net3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.net4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.net2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.net3.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.net4.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "    def reset(self):\n",
    "        self.net.apply(lambda m: torch.nn.init.kaiming_normal_(m.weight) if isinstance(m, nn.Conv2d) else None)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        self.reset()\n",
    "        input = self.net(input)\n",
    "        target = self.net(target)\n",
    "        net_loss = self.loss_fn(input, target)\n",
    "        input = self.net2(input)\n",
    "        target = self.net2(target)\n",
    "        net_loss += self.loss_fn(input, target) * 2\n",
    "        input = self.net3(input)\n",
    "        target = self.net3(target)\n",
    "        net_loss += self.loss_fn(input, target) * 4\n",
    "        input = self.net4(input)\n",
    "        target = self.net4(target)\n",
    "        net_loss += self.loss_fn(input, target) * 8\n",
    "        return net_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPIPSLoss(nn.Module):\n",
    "    def __init__(self, net='alex'):\n",
    "        super().__init__()\n",
    "        self.loss = lpips.LPIPS(net=net)\n",
    "        for param in self.loss.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Normalize input and target to [-1, 1]\n",
    "        input = torch.clamp(input, 0, 1)\n",
    "        target = torch.clamp(target, 0, 1)\n",
    "        input = input * 2 - 1\n",
    "        target = target * 2 - 1\n",
    "        \n",
    "        return self.loss(input, target).squeeze().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lpipsloss = LPIPSLoss()\n",
    "        self.mseloss = nn.MSELoss()\n",
    "    def forward(self, input, target):\n",
    "        return 0.8 * self.mseloss(input, target) + 0.2 * self.lpipsloss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSNRloss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return 10 * torch.log10((torch.max(target) - torch.min(target))**2 / F.mse_loss(pred, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = BaseLineUnet()\n",
    "        self.model_code = create_dict_of_files()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        checkpoint[\"model_code\"] = self.model_code\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint):\n",
    "        self.model_code = checkpoint[\"model_code\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        # if batch_idx == 0:\n",
    "        #     grid_x = torchvision.utils.make_grid(torch.clamp(x, 0, 1))\n",
    "        #     self.logger.experiment.add_image(\"train_x\", grid_x, self.global_step)\n",
    "        #     grid_y = torchvision.utils.make_grid(torch.clamp(y, 0, 1))\n",
    "        #     self.logger.experiment.add_image(\"train_y\", grid_y, self.global_step)\n",
    "        #     grid_y_hat = torchvision.utils.make_grid(torch.clamp(y_hat, 0, 1))\n",
    "        #     self.logger.experiment.add_image(\"train_y_hat\", grid_y_hat, self.global_step)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 200, eta_min=1e-6)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,  \n",
    "                'interval': 'epoch',   \n",
    "                'frequency': 1,         \n",
    "                'monitor': 'train_loss',   \n",
    "                'strict': True        \n",
    "            }\n",
    "        }\n",
    "    \n",
    "model = LightningWrapper()\n",
    "summary(model, (16, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GoProDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=11, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='train_loss', filename='best-checkpoint-{epoch:02d}-{train_loss:.6f}', save_last=True)\n",
    "lr_callback = LearningRateMonitor(logging_interval='epoch')\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "trainer = L.Trainer(max_epochs=200, precision='bf16-mixed', callbacks=[checkpoint_callback, lr_callback])\n",
    "trainer.fit(model, train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
