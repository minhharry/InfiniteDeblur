{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "from dataset import GoProDataset\n",
    "from model import BaseLineUnet\n",
    "from NAFnet_baseline.Baseline_arch import Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_of_files():\n",
    "    files = []\n",
    "    files.extend(glob.glob(\"**/*.py\", recursive=True))\n",
    "    files.extend(glob.glob(\"**/*.ipynb\", recursive=True))\n",
    "    data = {}\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text_content = f.read()\n",
    "        data[file] = text_content\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import LPIPSLoss\n",
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.lpipsloss = LPIPSLoss()\n",
    "    def forward(self, input, target):\n",
    "        return 0.8 * self.mse(input, target) + 0.2 * self.lpipsloss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import ResizeLoss\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = BaseLineUnet()\n",
    "        self.model_code = create_dict_of_files()\n",
    "        self.loss_fn = ResizeLoss()\n",
    "    \n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        checkpoint[\"model_code\"] = self.model_code\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint):\n",
    "        self.model_code = checkpoint[\"model_code\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        # if batch_idx == 0:\n",
    "        #     grid_x = torchvision.utils.make_grid(torch.clamp(x, 0, 1))\n",
    "        #     self.logger.experiment.add_image(\"train_x\", grid_x, self.global_step)\n",
    "        #     grid_y = torchvision.utils.make_grid(torch.clamp(y, 0, 1))\n",
    "        #     self.logger.experiment.add_image(\"train_y\", grid_y, self.global_step)\n",
    "        #     grid_y_hat = torchvision.utils.make_grid(torch.clamp(y_hat, 0, 1))\n",
    "        #     self.logger.experiment.add_image(\"train_y_hat\", grid_y_hat, self.global_step)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 500, eta_min=1e-6)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,  \n",
    "                'interval': 'epoch',   \n",
    "                'frequency': 1,         \n",
    "                'monitor': 'train_loss',   \n",
    "                'strict': True        \n",
    "            }\n",
    "        }\n",
    "    \n",
    "model = LightningWrapper()\n",
    "summary(model, (16, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PRETRAINED = False\n",
    "if LOAD_PRETRAINED:\n",
    "    checkpoint = torch.load(\"\")\n",
    "    model_weights = checkpoint[\"state_dict\"]\n",
    "    for key in list(model_weights):\n",
    "        model_weights[key.replace(\"model.\", \"\")] = model_weights.pop(key)\n",
    "    for key in list(model_weights):\n",
    "        if key.startswith(\"loss_fn.\"):\n",
    "            model_weights.pop(key)\n",
    "    model.model.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GoProDataset(root_dir='E:\\\\Downloads\\\\GOPRO_Large\\\\train', addnoise=False, mode='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=11, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='train_loss', filename='best-checkpoint-{epoch:02d}-{train_loss:.6f}', save_last=True)\n",
    "lr_callback = LearningRateMonitor(logging_interval='epoch')\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "trainer = L.Trainer(max_epochs=500, precision='bf16-mixed', callbacks=[checkpoint_callback, lr_callback])\n",
    "trainer.fit(model, train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
